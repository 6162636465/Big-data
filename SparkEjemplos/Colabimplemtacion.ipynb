{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKUDHjlNvN1p"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNY4NS1pgk9N"
      },
      "source": [
        "## Instalación de paquetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io8C2VOBrhXI",
        "outputId": "96278647-b19e-41f4-816c-970d345de135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!apt-get update  > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.3-bin-hadoop3.tgz\n",
        "!pip install findspark pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzeYjQfxyqvb",
        "outputId": "26848f03-b9fe-449d-cea2-6c49b8b93fd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-3.5.3-bin-hadoop3  spark-3.5.3-bin-hadoop3.tgz\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6uaGuaAvePq"
      },
      "source": [
        "## Preparación del ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o636IcyBGXA8",
        "outputId": "c9111df6-ec97-4cd6-d79b-ee11591232c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONHASHSEED=1234\n",
            "env: JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
            "env: SPARK_HOME=/content/spark-3.5.3-bin-hadoop3\n"
          ]
        }
      ],
      "source": [
        "%env PYTHONHASHSEED=1234\n",
        "%env JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
        "%env SPARK_HOME=/content/spark-3.5.3-bin-hadoop3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fv5l3EeymOj",
        "outputId": "71003e24-ac5e-420f-ac43-2992b1c2258e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin   data\tjars\t    LICENSE   NOTICE  R\t\t RELEASE  yarn\n",
            "conf  examples\tkubernetes  licenses  python  README.md  sbin\n"
          ]
        }
      ],
      "source": [
        "!ls $SPARK_HOME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "W0gLXPcaY8L6"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init(\"/content/spark-3.5.3-bin-hadoop3\")\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"MobyDickSearch\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Txt a buscar"
      ],
      "metadata": {
        "id": "hTIEYnLQn47a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEUnl2bTpZ6d",
        "outputId": "72ffa962-c592-44c3-e964-0e442d9adda7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3gvF0eRT982N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc66374e-0542-4595-e571-9b77b82016e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Líneas que contienen la palabra \"whale\": 1620\n"
          ]
        }
      ],
      "source": [
        "# Leer el archivo como RDD\n",
        "rdd = spark.sparkContext.textFile('drive/MyDrive/bigdata/mobydick.txt')\n",
        "# Filtrar las líneas que contienen la palabra \"whale\"\n",
        "keyword = \"whale\"\n",
        "filtered = rdd.filter(lambda line: keyword.lower() in line.lower())\n",
        "# Mapear cada línea encontrada a 1\n",
        "ones = filtered.map(lambda line: 1)\n",
        "# Reducir para contar la cantidad total de líneas que contienen la palabra\n",
        "count = ones.reduce(lambda a, b: a + b)\n",
        "\n",
        "print(f'Líneas que contienen la palabra \"{keyword}\": {count}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regresión Logística\n",
        "Generar datos sintéticos"
      ],
      "metadata": {
        "id": "cAHI4nyUqA3u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9JMO_yBCj6RF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Función para generar puntos sintéticos\n",
        "def generate_point(dim=10):\n",
        "    x = np.random.randn(dim)\n",
        "    y = 1 if np.dot(x, np.ones(dim)) + np.random.randn() > 0 else -1\n",
        "    return (x.tolist(), y)\n",
        "\n",
        "# Generar 10,000 puntos sintéticos en dimensión 10\n",
        "points_list = [generate_point(10) for _ in range(10000)]\n",
        "\n",
        "# Crear RDD\n",
        "rdd_points = spark.sparkContext.parallelize(points_list).cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparar descenso por gradiente con acumuladores"
      ],
      "metadata": {
        "id": "XrumWd5KqdGR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qa3Ea2vsFc8v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Función para generar puntos sintéticos\n",
        "def generate_point(dim=10):\n",
        "    x = np.random.randn(dim)\n",
        "    y = 1 if np.dot(x, np.ones(dim)) + np.random.randn() > 0 else -1\n",
        "    return (x.tolist(), y)\n",
        "\n",
        "# Crear 10,000 puntos sintéticos\n",
        "points_list = [generate_point(10) for _ in range(10000)]\n",
        "\n",
        "# Crear RDD\n",
        "rdd_points = spark.sparkContext.parallelize(points_list).cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funciones auxiliares estables numéricamente"
      ],
      "metadata": {
        "id": "gd8VhoYKq60W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import AccumulatorParam\n",
        "import math\n",
        "\n",
        "D = 10  # dimensión del vector\n",
        "ITERATIONS = 10\n",
        "LR = 0.1  # tasa de aprendizaje\n",
        "\n",
        "# Inicializar vector de pesos aleatorio\n",
        "w = np.random.randn(D)\n",
        "\n",
        "# Clase acumuladora para vectores\n",
        "class VectorAccumulatorParam(AccumulatorParam):\n",
        "    def zero(self, value):\n",
        "        return [0.0] * len(value)\n",
        "    def addInPlace(self, val1, val2):\n",
        "        return [v1 + v2 for v1, v2 in zip(val1, val2)]\n",
        "\n",
        "# Registrar acumulador\n",
        "grad_acc = spark.sparkContext.accumulator([0.0]*D, VectorAccumulatorParam())\n"
      ],
      "metadata": {
        "id": "3aWd3ApSprD_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenar el modelo"
      ],
      "metadata": {
        "id": "YFFEvhkvxOgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función sigmoide con protección contra overflow\n",
        "def sigmoid(z):\n",
        "    if z < -20:\n",
        "        return 0.0\n",
        "    elif z > 20:\n",
        "        return 1.0\n",
        "    else:\n",
        "        return 1 / (1 + math.exp(-z))\n",
        "\n",
        "# Función de gradiente para un punto\n",
        "def gradient(p):\n",
        "    x, y = p\n",
        "    dot = sum(w[j] * x[j] for j in range(D))\n",
        "    s = (sigmoid(dot * y) - 1) * y\n",
        "    return [s * x[j] for j in range(D)]\n",
        "\n",
        "for i in range(ITERATIONS):\n",
        "    grad_acc.value = [0.0]*D  # reiniciar acumulador\n",
        "    rdd_points.foreach(lambda p: grad_acc.add(gradient(p)))\n",
        "    w = [w[j] - LR * grad_acc.value[j] for j in range(D)]\n",
        "    print(f\"Iteración {i+1} completada. ||grad|| = {round(np.linalg.norm(grad_acc.value), 4)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fyuxx3p2ptwn",
        "outputId": "078819eb-fc03-4bc6-b83d-e70ec4e830fa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 1 completada. ||grad|| = 178.3077\n",
            "Iteración 2 completada. ||grad|| = 179.9626\n",
            "Iteración 3 completada. ||grad|| = 182.4941\n",
            "Iteración 4 completada. ||grad|| = 185.6186\n",
            "Iteración 5 completada. ||grad|| = 190.6954\n",
            "Iteración 6 completada. ||grad|| = 195.9549\n",
            "Iteración 7 completada. ||grad|| = 208.9553\n",
            "Iteración 8 completada. ||grad|| = 236.6531\n",
            "Iteración 9 completada. ||grad|| = 319.7692\n",
            "Iteración 10 completada. ||grad|| = 545.6071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluar el modelo"
      ],
      "metadata": {
        "id": "b8C9piNTxRHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función de predicción\n",
        "def predict(x):\n",
        "    return 1 if sum(w[j] * x[j] for j in range(D)) >= 0 else -1\n",
        "\n",
        "# Calcular precisión\n",
        "accuracy = rdd_points.map(lambda p: 1 if predict(p[0]) == p[1] else 0).mean()\n",
        "print(f\"Precisión en datos de entrenamiento: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weRtEQXZwXzD",
        "outputId": "d542fc49-e9b2-49c6-cc48-d8dc537d7e7b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión en datos de entrenamiento: 0.8718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ALS"
      ],
      "metadata": {
        "id": "6JYF1w_0wjEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "# Creamos un pequeño conjunto de datos de ejemplo: userId, itemId, rating\n",
        "data = [\n",
        "    Row(userId=0, itemId=0, rating=4.0),\n",
        "    Row(userId=0, itemId=1, rating=2.0),\n",
        "    Row(userId=1, itemId=1, rating=3.0),\n",
        "    Row(userId=1, itemId=2, rating=4.0),\n",
        "    Row(userId=2, itemId=2, rating=5.0),\n",
        "    Row(userId=2, itemId=0, rating=3.0)\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TDZLNLWwkLx",
        "outputId": "6112595f-9b40-4d70-ea08-d37dddf62196"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+------+\n",
            "|userId|itemId|rating|\n",
            "+------+------+------+\n",
            "|     0|     0|   4.0|\n",
            "|     0|     1|   2.0|\n",
            "|     1|     1|   3.0|\n",
            "|     1|     2|   4.0|\n",
            "|     2|     2|   5.0|\n",
            "|     2|     0|   3.0|\n",
            "+------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenar el modelo ALS"
      ],
      "metadata": {
        "id": "mI0EagYLxqYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.recommendation import ALS\n",
        "\n",
        "als = ALS(\n",
        "    userCol=\"userId\",\n",
        "    itemCol=\"itemId\",\n",
        "    ratingCol=\"rating\",\n",
        "    maxIter=10,\n",
        "    regParam=0.1,\n",
        "    rank=5,\n",
        "    coldStartStrategy=\"drop\"  # evita NaN en predicciones\n",
        ")\n",
        "\n",
        "model = als.fit(df)\n"
      ],
      "metadata": {
        "id": "Lelr3fsmwqih"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacer predicciones"
      ],
      "metadata": {
        "id": "e-RYOaW-xuah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear combinaciones usuario-producto para predecir\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "users = df.select(\"userId\").distinct()\n",
        "items = df.select(\"itemId\").distinct()\n",
        "\n",
        "user_item_pairs = users.crossJoin(items)\n",
        "\n",
        "predictions = model.transform(user_item_pairs)\n",
        "predictions.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4i8tt2_qwxz5",
        "outputId": "96b8fa25-e418-4dbd-d796-46459ec1ee5d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+----------+\n",
            "|userId|itemId|prediction|\n",
            "+------+------+----------+\n",
            "|     0|     0|  3.837964|\n",
            "|     1|     0| 2.5495687|\n",
            "|     2|     0| 3.0063426|\n",
            "|     0|     1| 2.0184698|\n",
            "|     1|     1| 2.8155203|\n",
            "|     2|     1| 2.8713424|\n",
            "|     0|     2| 3.4623556|\n",
            "|     1|     2|  4.003248|\n",
            "|     2|     2| 4.8675914|\n",
            "+------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extraer recomendaciones"
      ],
      "metadata": {
        "id": "79nmMrOfxxup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recomendaciones de películas para cada usuario\n",
        "model.recommendForAllUsers(numItems=3).show(truncate=False)\n",
        "\n",
        "# Recomendaciones de usuarios para cada producto\n",
        "model.recommendForAllItems(numUsers=3).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-8qwZ_Yw3xB",
        "outputId": "48e4bb3f-b09f-404d-f88e-013a61a59641"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------------------------------------------+\n",
            "|userId|recommendations                                 |\n",
            "+------+------------------------------------------------+\n",
            "|0     |[{0, 3.837964}, {2, 3.4623556}, {1, 2.0184698}] |\n",
            "|1     |[{2, 4.003248}, {1, 2.8155203}, {0, 2.5495687}] |\n",
            "|2     |[{2, 4.8675914}, {0, 3.0063426}, {1, 2.8713424}]|\n",
            "+------+------------------------------------------------+\n",
            "\n",
            "+------+------------------------------------------------+\n",
            "|itemId|recommendations                                 |\n",
            "+------+------------------------------------------------+\n",
            "|0     |[{0, 3.837964}, {2, 3.0063426}, {1, 2.5495687}] |\n",
            "|1     |[{2, 2.8713424}, {1, 2.8155203}, {0, 2.0184698}]|\n",
            "|2     |[{2, 4.8675914}, {1, 4.003248}, {0, 3.4623556}] |\n",
            "+------+------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}